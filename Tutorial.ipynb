{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dependency\n",
    "\n",
    "- [nltk](https://www.nltk.org)\n",
    "- [sklearn](http://scikit-learn.org/stable/)\n",
    "- [word2vec](https://github.com/danielfrg/word2vec)\n",
    "- [fastText](https://fasttext.cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data ...\n",
      "40000 40000\n",
      "Development data ...\n",
      "5000 5000\n",
      "Test data ...\n",
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "trn_texts = open(\"trn-reviews.txt\").read().strip().split(\"\\n\")\n",
    "trn_labels = map(str, open(\"trn-labels.txt\").read().strip().split(\"\\n\"))\n",
    "print \"Training data ...\"\n",
    "print len(trn_texts), len(trn_labels)\n",
    "\n",
    "dev_texts = open(\"dev-reviews.txt\").read().strip().split(\"\\n\")\n",
    "dev_labels = map(str, open(\"dev-labels.txt\").read().strip().split(\"\\n\"))\n",
    "print \"Development data ...\"\n",
    "print len(dev_texts), len(dev_labels)\n",
    "\n",
    "tst_texts = open(\"tst-reviews.txt\").read().strip().split(\"\\n\")\n",
    "tst_labels = map(str, open(\"tst-labels.txt\").read().strip().split(\"\\n\"))\n",
    "print \"Test data ...\"\n",
    "print len(tst_texts), len(tst_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the training data with different choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercasing and filtering out low-frequency words\n",
      "(40000, 31218)\n"
     ]
    }
   ],
   "source": [
    "choice = 3\n",
    "\n",
    "if choice == 1:\n",
    "    print \"Preprocessing without any feature selection\"\n",
    "    vectorizer = CountVectorizer(lowercase=False)\n",
    "    # vocab size 77166\n",
    "elif choice == 2:\n",
    "    print \"Lowercasing all the tokens\"\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    # vocab size 60610\n",
    "elif choice == 3:\n",
    "    print \"Lowercasing and filtering out low-frequency words\"\n",
    "    vectorizer = CountVectorizer(lowercase=True, min_df=2)\n",
    "    # vocab size 31218\n",
    "elif choice == 4:\n",
    "    print \"Lowercasing and filtering out low-frequency words, uni- and bi-gram\"\n",
    "    vectorizer = CountVectorizer(lowercase=True, min_df=2, ngram_range=(1,2))\n",
    "    # vocab size 323167\n",
    "elif choice == 5:\n",
    "    print \"Uni- and bi-gram\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "    # vocab 1048596\n",
    "elif choice == 6:\n",
    "    print \"Lowercasing and filtering out high-frequency words\"\n",
    "    vectorizer = CountVectorizer(lowercase=True, max_df=0.5)\n",
    "    # vocab size 60610\n",
    "\n",
    "trn_data = vectorizer.fit_transform(trn_texts)\n",
    "print trn_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 31218)\n",
      "(5000, 31218)\n"
     ]
    }
   ],
   "source": [
    "dev_data = vectorizer.transform(dev_texts)\n",
    "print dev_data.shape\n",
    "tst_data = vectorizer.transform(tst_texts)\n",
    "print tst_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a LR classifier\n",
    "\n",
    "classifier = LR(C=10000, solver=\"lbfgs\")\n",
    "classifier.fit(trn_data, trn_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy = 0.913325\n",
      "Dev accuracy = 0.5754\n"
     ]
    }
   ],
   "source": [
    "# Measure the performance on training and dev data\n",
    "print \"Training accuracy =\", classifier.score(trn_data, trn_labels)\n",
    "print \"Dev accuracy =\", classifier.score(dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Choice 1, C=10000, solver='lbfgs'\n",
    "\n",
    "- Trn acc = 0.8492\n",
    "- Dev acc = 0.6056\n",
    "\n",
    "Setup: Choice 2, C=10000, solver='lbfgs'\n",
    "\n",
    "- Trn acc = 0.8213\n",
    "- Dev acc = 0.6142\n",
    "\n",
    "Setup: Choice 3, C=10000, solver='lbfgs'\n",
    "\n",
    "- Trn acc = 0.8099\n",
    "- Dev acc = 0.6174\n",
    "\n",
    "Setup: Choice 4, C=10000, solver='lbfgs'\n",
    "\n",
    "- Trn acc = 0.9978\n",
    "- Dev acc = 0.6202\n",
    "\n",
    "Setup: Choice 5, C=10000, solver='lbfgs'\n",
    "\n",
    "- Trn acc = 0.9999\n",
    "- Dev acc = 0.624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup: choice = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get vocab\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "ivocab = {}\n",
    "for (key, val) in vocab.iteritems():\n",
    "    ivocab[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get classification weights\n",
    "\n",
    "weights = classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.60375083477\n"
     ]
    }
   ],
   "source": [
    "print weights[0][vocab['delicious']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get salient features for each class\n",
    "\n",
    "from numpy import argsort\n",
    "\n",
    "def get_top_features(weight, vocab, topn=10):\n",
    "    sorted_indices = list(argsort(weight))[::-1]\n",
    "    for n in range(topn):\n",
    "        print ivocab[sorted_indices[n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptional\n",
      "incredible\n",
      "phenomenal\n",
      "body\n",
      "regret\n",
      "worried\n",
      "skeptical\n",
      "hesitate\n",
      "happier\n",
      "mike\n"
     ]
    }
   ],
   "source": [
    "# top features with user rating 5\n",
    "get_top_features(weights[4], ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst\n",
      "joke\n",
      "disgusted\n",
      "unprofessional\n",
      "garbage\n",
      "disgusting\n",
      "luck\n",
      "pathetic\n",
      "apologies\n",
      "horrible\n"
     ]
    }
   ],
   "source": [
    "# top features with user rating 1\n",
    "get_top_features(weights[0], ivocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word2vec demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import word2vec\n",
    "\n",
    "# more examples in http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def preprocessing_for_word2vec(fname):\n",
    "    text = open(fname).read().replace(\"\\n\",\" \")\n",
    "    text = \" \".join(wordpunct_tokenize(text)).lower()\n",
    "    with open('word2vec-input.txt', 'w') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "word2vec_data = preprocessing_for_word2vec(\"trn-reviews.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file word2vec-input.txt\n",
      "Vocab size: 19026\n",
      "Words in train file: 5531279\n",
      "Alpha: 0.000002  Progress: 100.14%  Words/thread/sec: 530.87k  "
     ]
    }
   ],
   "source": [
    "word2vec.doc2vec(\"word2vec-input.txt\", \"word2vec-output.bin\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.load('word2vec-output.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'delicious', u'tasty', u'delish', u'yum', u'incredible', u'superb',\n",
       "       u'phenomenal', u'fantastic', u'disappoint', u'awesome'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = w2v_model.cosine('yummy')\n",
    "w2v_model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'terrible', u'poor', u'awful', u'customer', u'exceptional', u'bad',\n",
       "       u'astonished', u'pleasant', u'happier', u'zero'], \n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes, metrics = w2v_model.cosine('horrible')\n",
    "w2v_model.vocab[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reformat the data for fastText\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def reformat(infname, labelfname, outfname):\n",
    "    texts = open(infname).read().split(\"\\n\")\n",
    "    labels = open(labelfname).read().split(\"\\n\")\n",
    "    fout = open(outfname, 'w')\n",
    "    for (text, label) in zip(texts, labels):\n",
    "        fout.write(\"__label__{} {}\\n\".format(label, \" \".join(wordpunct_tokenize(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reformat(\"trn-reviews.txt\",\"trn-labels.txt\",\"trn-data-fasttext.txt\")\n",
    "reformat(\"dev-reviews.txt\",\"dev-labels.txt\",\"dev-data-fasttext.txt\")\n",
    "reformat(\"tst-reviews.txt\",\"tst-labels.txt\",\"tst-data-fasttext.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run fastText\n",
    "\n",
    "\n",
    "    ./fasttext supervised -input ../trn-data-fasttext.txt -output model \n",
    "    ./fasttext test model.bin ../trn-data-fasttext.txt\n",
    "    ./fasttext test model.bin ../dev-data-fasttext.txt\n",
    "\n",
    "\n",
    "- Trn acc = 0.654\n",
    "- Dev acc = 0.651\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
